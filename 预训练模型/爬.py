import requests
from bs4 import BeautifulSoup
import json
import time
from concurrent.futures import ThreadPoolExecutor

# âš™ï¸ é…ç½®åŒºåŸŸ
POSTS_TO_FETCH = 100         # ç›®æ ‡å¸–å­æ€»æ•°
THREADS_PER_PAGE = 20        # æ¯é¡µè¯·æ±‚çš„å¸–å­æ•°
OUTPUT_FILE = "tieba_posts.json"
MAX_WORKERS = 5              # å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé™ä½ä»¥å‡å°‘å°ç¦é£é™©ï¼‰

# ğŸ§¾ è¯·æ±‚å¤´é…ç½®ï¼ˆå« BDUSS/BDUTT ä»¥ç»•è¿‡åçˆ¬ï¼‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36",
    "Referer": "https://tieba.baidu.com/",
    "BDUSS": "your_BDUSS_cookie_here",  # âš ï¸ å¿…é¡»å¡«å†™æœ‰æ•ˆçš„ BDUSSï¼
    "BDUTT": "your_BDUTT_cookie_here"   # âš ï¸ å¿…é¡»å¡«å†™æœ‰æ•ˆçš„ BDUTTï¼
}

# ğŸ”— é¦–é¡µåœ°å€
INDEX_URL = "https://tieba.baidu.com/index.html"

def fetch_index_html():
    """
    è·å–è´´å§é¦–é¡µ HTML å†…å®¹
    """
    print(f"[è¯·æ±‚] æ­£åœ¨è¯·æ±‚é¦–é¡µ: {INDEX_URL}")
    try:
        resp = requests.get(INDEX_URL, headers=HEADERS, timeout=10)
        print(f"[å“åº”] çŠ¶æ€ç : {resp.status_code}, URL: {INDEX_URL}")
        if resp.status_code != 200:
            print(f"[é”™è¯¯] é¦–é¡µè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {resp.status_code}")
            print(f"[å“åº”å†…å®¹] {resp.text[:500]}...")
            return ""
        return resp.text
    except Exception as e:
        print(f"[é”™è¯¯] é¦–é¡µè¯·æ±‚å¤±è´¥: {e}")
        return ""

def parse_index_html(html):
    """
    ä»é¦–é¡µ HTML ä¸­è§£ææ¨èå¸–å­
    """
    soup = BeautifulSoup(html, "html.parser")
    threads = []

    # ğŸ§¹ æ–°ç‰ˆé¡µé¢ç»“æ„ï¼šæ¨èå¸–å­å®¹å™¨ç±»åä¸º .j_feed_li
    recommend_blocks = soup.select(".j_feed_li")
    if not recommend_blocks:
        print("[æç¤º] æœªæ‰¾åˆ°æ¨èå¸–å­å®¹å™¨ï¼Œé¡µé¢ç»“æ„å¯èƒ½å·²å˜æ›´")
        return threads

    for block in recommend_blocks:
        title_elem = block.select_one(".n_right a")  # å‡è®¾æ ‡é¢˜é“¾æ¥ä½äº .n_right ä¸‹çš„ç¬¬ä¸€ä¸ª <a> æ ‡ç­¾
        thread_id = block.get("data-thread-id")

        if not title_elem or not thread_id:
            continue

        title = title_elem.get_text(strip=True)

        # ğŸ·ï¸ æå–æ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        tags = []
        for tag in block.select(".post_tag"):
            tag_text = tag.get_text(strip=True)
            if tag_text:
                tags.append(tag_text)

        threads.append({
            "thread_id": thread_id,
            "title": title,
            "tags": tags
        })

    print(f"[è§£æ] å…±æ‰¾åˆ° {len(threads)} ä¸ªæ¨èä¸»é¢˜")
    return threads

def fetch_content(thread_id):
    """
    è·å–æŒ‡å®šå¸–å­çš„é¦–æ¥¼å†…å®¹
    """
    try:
        url = f"https://tieba.baidu.com/p/{thread_id}?see_lz=1"
        print(f"[è¯·æ±‚] æ­£åœ¨è·å–å¸–å­å†…å®¹: {url}")
        resp = requests.get(url, headers=HEADERS, timeout=10)
        print(f"[å“åº”] çŠ¶æ€ç : {resp.status_code}, URL: {url}")

        if resp.status_code != 200:
            print(f"[é”™è¯¯] é 200 å“åº”ï¼ŒçŠ¶æ€ç : {resp.status_code}, URL: {url}")
            return ""

        soup = BeautifulSoup(resp.text, "html.parser")
        content_elem = soup.select_one(".p_content")
        if not content_elem:
            content_elem = soup.select_one(".post_content")

        content = content_elem.get_text(strip=True) if content_elem else ""
        print(f"[å†…å®¹] å¸–å­ {thread_id} å·²æˆåŠŸè·å–æ­£æ–‡")
        return content

    except Exception as e:
        print(f"[é”™è¯¯] å¸–å­ {thread_id} è·å–å¤±è´¥: {e}, URL: {url}")
        return ""

def main():
    print("âš ï¸ æ³¨æ„ï¼šå¿…é¡»å¡«å†™æœ‰æ•ˆçš„ BDUSS å’Œ BDUTT Cookieï¼")
    html = fetch_index_html()
    if not html:
        print("[é”™è¯¯] æ— æ³•è·å–é¦–é¡µ HTMLï¼Œç¨‹åºç»ˆæ­¢")
        return

    # 1ï¸âƒ£ è§£æé¦–é¡µæ¨èå¸–å­
    threads = parse_index_html(html)
    if not threads:
        print("[é”™è¯¯] æœªè§£æåˆ°ä»»ä½•æ¨èå¸–å­")
        return

    # 2ï¸âƒ£ å¹¶å‘è·å–æ­£æ–‡å†…å®¹
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        contents = list(executor.map(lambda th: fetch_content(th["thread_id"]), threads))

    # 3ï¸âƒ£ åˆå¹¶æ•°æ®å¹¶ä¿å­˜
    for i, thread in enumerate(threads):
        thread["content"] = contents[i]

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(threads, f, ensure_ascii=False, indent=2)

    print(f"ğŸ‰ å·²ä¿å­˜åˆ° {OUTPUT_FILE}")

if __name__ == "__main__":
    main()