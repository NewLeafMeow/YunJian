import jieba
import joblib
import os

def chinese_tokenizer(text):
    """å¯¹ä¸­æ–‡æ–‡æœ¬è¿›è¡Œåˆ†è¯"""
    return " ".join(jieba.cut(text))

def predict_new_text(text):
    model_path = 'calibrated_svm_model.pkl'
    vec_path = 'tfidf_vectorizer.pkl'

    if not os.path.exists(model_path) or not os.path.exists(vec_path):
        raise FileNotFoundError("æ¨¡å‹æˆ–å‘é‡åŒ–å™¨æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒéƒ¨åˆ†")

    calibrated_clf = joblib.load(model_path)
    vectorizer = joblib.load(vec_path)

    # åˆ†è¯å¹¶è½¬æ¢ä¸º TF-IDF ç‰¹å¾å‘é‡
    tokenized_text = chinese_tokenizer(text)
    text_vec = vectorizer.transform([tokenized_text])

    # é¢„æµ‹ç±»åˆ«å’Œæ¦‚ç‡
    prediction = calibrated_clf.predict(text_vec)
    probabilities = calibrated_clf.predict_proba(text_vec)

    # è¿”å›é¢„æµ‹ç»“æœå’Œå„ç±»åˆ«æ¦‚ç‡
    return prediction[0], dict(zip(calibrated_clf.classes_, probabilities[0]))

# ======================================
# ğŸ§ª ç¤ºä¾‹æµ‹è¯•ï¼šå¯¹æ–°æ–‡æœ¬è¿›è¡Œé¢„æµ‹
# ======================================
if __name__ == '__main__':
    sample_text = "è¿™æ˜¯ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„ç§‘æŠ€æ–°é—»ã€‚"
    predicted_label, confidence_scores = predict_new_text(sample_text)

    print(f"è¾“å…¥æ–‡æœ¬: {sample_text}")
    print(f"é¢„æµ‹ç±»åˆ«: {predicted_label}")
    print("å„ç±»åˆ«åŒ¹é…åº¦ï¼ˆæ¦‚ç‡ï¼‰:")
    for label, score in confidence_scores.items():
        print(f"  - {label}: {score:.4f}")